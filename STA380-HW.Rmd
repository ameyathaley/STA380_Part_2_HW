---
title: "Untitled"
output: github_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<TABLE>
<TH>Names</TD>
<TR><TD>Aishwarya Pawar</TD></TR>
<TR><TD>Amey Athaley</TD></TR>
<TR><TD>Kachi Ugo</TD></TR>
<TR><TD>Sadhana Koneni</TD></TR>
</TABLE>

--------------------------------------------------------------------------------------  
  
# Question 1 - Green Buildings
  
  
--------------------------------------------------------------------------------------

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(mosaic)

buildings = read.csv('C:/Users/ameya/Documents/UT Austin/Academic/Intro to Predictive Modelling/STA380-master/STA380-master/data/greenbuildings.csv')

# only empl_gr - the year on year employment growth in the geographic region - has 74 null values among all columns
sapply(buildings,function(x) sum(is.na(x)))

# very low leasing rates - outlier treatment
ggplot(buildings) +  
  geom_point(aes(x=leasing_rate, y = Rent, fill = factor(green_rating)), shape = 21, alpha = 0.5, size = 2) + 
  theme_classic()
```
--------------------------------------------------------------------------------------   
## Outlier Treatment:
Like the analyst mentioned, we observe here that the there are few buildings with very low occupancy rates. These buildings also have rents which are varying. These 215 buildings have been removed from further analysis as these could potentially distort the analysis
  
  
--------------------------------------------------------------------------------------

 

```{r message=FALSE, warning=FALSE}
dim(buildings)
buildings = buildings[buildings$leasing_rate > 10,]
dim(buildings)


## Calculating the premium on the green buildings - $27.6-$25 which is $2.6
buildings_summ = buildings %>%
  group_by(green_rating)  %>%  # group the data points by green_rating
  summarize(rent.mean = median(Rent))
buildings_summ

# rent distribution in green and non-green buildings: Green buildings have slightly higher rents
ggplot(data=buildings) + 
  geom_boxplot(mapping=aes(x=factor(green_rating),y=log(Rent)))

```

--------------------------------------------------------------------------------------   
## Class
Our hypothesis is that the rents are higher for Class A buildings
  
  
--------------------------------------------------------------------------------------


```{r message=FALSE, warning=FALSE}

###########********** 1. Class **********###########

buildings$class = ifelse(buildings$class_a == 1, "A", ifelse(buildings$class_b == 1,"B","C") )

buildings_class = buildings %>% group_by(class, green_rating) %>% summarize(rent.median = median(Rent), rent.mean= mean(Rent))

ggplot(data = buildings_class) + 
  geom_bar(mapping = aes(x=class, y=rent.mean, fill=factor(green_rating)),
           position="dodge", stat='identity')

table(buildings$class, buildings$green_rating)


```


--------------------------------------------------------------------------------------   
### Findings: 
We observe that the class of the building makes a difference but green vs. non-green buildings doesn't show any significant changes in rent except for Class C. On further investigation, we find that there are only 7 green buildings so the average in this case may not be an accurate representation.  


  
--------------------------------------------------------------------------------------
  
## Age
  
--------------------------------------------------------------------------------------

```{r message=FALSE, warning=FALSE}
###########********** 2. Age **********###########
buildings_age = buildings %>%
  mutate(agecat = cut(age, c(-1, 20, 40, 60, 80, 100, 120, 140, 160, 180, 200)))
summary(buildings_age)

buildings_age %>%
  group_by(agecat, green_rating)  %>%  # group the data points by age category
  summarize(rent.mean = mean(Rent), rent.median = median(Rent))

table(buildings_age$agecat, buildings_age$green_rating)

ggplot(data = buildings) + 
  geom_histogram(mapping = aes(x=age, y=stat(density), fill=factor(green_rating)), binwidth = 3)

ggplot(buildings_age) +
  geom_boxplot(mapping=aes(x=agecat,y=Rent, fill=factor(green_rating)))
```
  
--------------------------------------------------------------------------------------
  
### Findings:
Most of the green buildings are less than 4 decades old and we see that as the buildings get older, green buildings tend to have higher rents compared to non-green buildings. But in the initial decades, there is no descernible difference in rents between green and non-green categories.
  
-------------------------------------------------------------------------------------
  
  
## Amenities
  
  
-------------------------------------------------------------------------------------

```{r message=FALSE, warning=FALSE}
###########********** 3. Amenities **********###########
buildings %>%
  group_by(amenities)  %>%  # group the data points by amenities
  summarize(rent.mean = mean(Rent), rent.median = median(Rent))

buildings_am = buildings %>%
  group_by(green_rating) %>%
  summarize(am_pct = sum(amenities==1)/n())
buildings_am

ggplot(data = buildings_am) + 
  geom_bar(mapping = aes(x=green_rating, y=am_pct), stat='identity')

```

-------------------------------------------------------------------------------------
  
Our hypothesis here was that buildings with amenities would have higher rent and if green buildings have higher rents within the amenities vs. non-amenities, premium can be attributed to the fact that the building is green. 
  
-------------------------------------------------------------------------------------

-------------------------------------------------------------------------------------
  
### Findings:
We observed that the mean rent is higher for green buildings with amenities. But, there are very few buildings with amenities in our dataset and we have no information on the extra cost for providing these amenities. Hence, amenities has been ruled out as a factor in our analysis. 
  
-------------------------------------------------------------------------------------

-------------------------------------------------------------------------------------
  
  
## Net Contracts
  
  
-------------------------------------------------------------------------------------



```{r message=FALSE, warning=FALSE}
###########********** 4. Net contracts **********########### 
buildings %>%
  group_by(net)  %>%  # group the data points by net contract
  summarize(rent.mean = mean(Rent), rent.median = median(Rent))

buildings_net = buildings %>%
  group_by(green_rating) %>%
  summarize(net_pct = sum(net==1)/n())
buildings_net # green_rating = 1 -> 0.056, green_rating = 0 -> 0.032

ggplot(data = buildings_net) + 
  geom_bar(mapping = aes(x=green_rating, y=net_pct), stat='identity')
```

-------------------------------------------------------------------------------------
  
  
### Findings:
<OL>
  <LI>Rent is lower when it is a net-contract
  <LI>The net-contract buildings form very small percentage of the datapoints(<1%) and we do not have information on the utilities bill. 
  <LI>Thus, we omitted this from our analysis.
</OL>
  
-------------------------------------------------------------------------------------

-------------------------------------------------------------------------------------

## Leasing Rate
  
-------------------------------------------------------------------------------------

```{R message=FALSE, warning=FALSE}
###########********** 5. Leasing Rate **********########### 
# The leasing rate is higher for green buildings
ggplot(data=buildings) + 
  geom_boxplot(mapping=aes(x=factor(green_rating),y=leasing_rate))

leasing_rate_green = mean(buildings[buildings$green_rating==1,]$leasing_rate)
leasing_rate_nongreen = mean(buildings[buildings$green_rating==0,]$leasing_rate)

print(paste("The leasing rate for green buildings is", leasing_rate_green, "and for non-green buildings is", leasing_rate_nongreen))

```

-------------------------------------------------------------------------------------

## Conclusion:
The leasing rates are different for green and non-green buildings. Once we account for the leasing rate in the calculation, we see that the extra cost for green buildings can be recovered in 8.6 years(5mn/(premium for green buildings x leasing rate for green buildings x square footage)).

-------------------------------------------------------------------------------------


# Question 5 - Author Attribution
  
  
-------------------------------------------------------------------------------------

## Loading necessary libraries
  

-------------------------------------------------------------------------------------

```{r echo = F}
library(tm) 
library(magrittr)
library(slam)
library(proxy)
library('e1071')  # for naive bayes model
library(caret)
```
  

-------------------------------------------------------------------------------------
  
## Reading Train and Test data from the files
  

-------------------------------------------------------------------------------------
  
```{r echo = T}
readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), 
            id=fname, language='en') 
}
train_list = Sys.glob('C:/Users/ameya/Documents/UT Austin/Academic/Intro to Predictive Modelling/STA380-master/STA380-master/data/ReutersC50/C50train/*/*.txt')
train_data = lapply(train_list, readerPlain) 
test_list = Sys.glob('C:/Users/ameya/Documents/UT Austin/Academic/Intro to Predictive Modelling/STA380-master/STA380-master/data/ReutersC50/C50test/*/*.txt')
test_data = lapply(test_list, readerPlain) 

#Function to clean the names :

clean_list = function(list1) 
{
   clean_lst<- list1 %>%
    { strsplit(., '/', fixed=TRUE) } %>%
    { lapply(., tail, n=2) } %>%
    { lapply(., paste0, collapse = '') } %>%
    unlist
   
   return(clean_lst)
}

train_names=clean_list(train_list)
names(train_data) = train_names

test_names=clean_list(test_list)
names(test_data) = test_names

# Clean Author name list :

author_train= clean_list(train_list)
author_train= gsub('[0-9]+', '', author_train)
author_train= gsub('newsML.txt', '', author_train)

author_test= clean_list(test_list) 
author_test= gsub('[0-9]+', '', author_test)
author_test= gsub('newsML.txt', '', author_test)

```
  

-------------------------------------------------------------------------------------

## Generating the Corpus for train and test data
  

-------------------------------------------------------------------------------------


```{r echo = T}
documents_raw_train = Corpus(VectorSource(train_data))
documents_raw_test = Corpus(VectorSource(test_data))
```


-------------------------------------------------------------------------------------

## Tokenization 
  
-------------------------------------------------------------------------------------


```{r echo = T, warning=FALSE}

#Function for text pre-processing 

text_pre_proc= function(dat1) 
{
  my_documents = dat1
  my_documents = tm_map(my_documents, content_transformer(tolower)) # make everything lowercase 
  my_documents = tm_map(my_documents, content_transformer(removeNumbers)) # remove numbers
  my_documents = tm_map(my_documents, content_transformer(removePunctuation)) # remove punctuation
  my_documents = tm_map(my_documents, content_transformer(stripWhitespace)) ## remove excess white-space
  my_documents = tm_map(my_documents, content_transformer(removeWords), stopwords("en"))  # remove stop words
  return(my_documents)
}

# Pre-processing for train data :

train_doc=text_pre_proc(documents_raw_train)
test_doc=text_pre_proc(documents_raw_test)

```


-------------------------------------------------------------------------------------
  
## Creating Doc-term-matrix and calculating TF-IDF weights
  
-------------------------------------------------------------------------------------
  
  
```{r echo = T, warning=FALSE}

## Function to create doc matrix and fund TF-IDF weights 

DTM_mat = DocumentTermMatrix(train_doc)  # Convert to matrix
DTM_mat = removeSparseTerms(DTM_mat, 0.95) # Removing the longtail terms (5%)


tf_idf_train = weightTfIdf(DTM_mat)

# A suboptimal but practical solution: ignore words you haven't seen before
# can do this by pre-specifying a dictionary in the construction of a DTM
DTM_test = DocumentTermMatrix(test_doc, control=list(dictionary=Terms(DTM_mat)))

tf_idf_test=weightTfIdf(DTM_test)


```
  

-------------------------------------------------------------------------------------

## Dimensionlity Reduction: Principal Component Analysis

-------------------------------------------------------------------------------------
  

```{r echo = T, warning=FALSE}

# Data Pre-Processing for PCA : 
#1. create a matrix 
#2. Remove the columns with 0 values

X_train = as.matrix(tf_idf_train)
X_test = as.matrix(tf_idf_test)

#Removing columns with entries with 0 values
scrub_cols = which(colSums(X_train) == 0)
scrub_cols_test = which(colSums(X_test) == 0)
X_train = X_train[,-scrub_cols]
X_test = X_test[,-scrub_cols_test]

# drop uncommon words
X_test = X_test[,intersect(colnames(X_test),colnames(X_train))]
X_train = X_train[,intersect(colnames(X_test),colnames(X_train))]

```
  

-------------------------------------------------------------------------------------

## Train PCA on training dataset and predict for test dataset

-------------------------------------------------------------------------------------
  
  
```{r echo = T, warning=FALSE}

pca_train = prcomp(X_train, scale=TRUE)
pca_test=predict(pca_train,newdata = X_test )

```
  

-------------------------------------------------------------------------------------

## Choosing number of PCs to be selected
  
-------------------------------------------------------------------------------------

```{R message=FALSE, warning=FALSE}
plot(pca_train,type='line')
#summary(pca_train)
vars <- apply(pca_train$x, 2, var)  
props <- vars / sum(vars)
cumsum(props)
```
  
- Choosing 75% varability hence taking 290 PCs (As both train and train1 )
  

-------------------------------------------------------------------------------------

## Create the final dataset with reduced dimensions and with authornames
  

-------------------------------------------------------------------------------------

```{r echo = T,warning=FALSE}
final_train = data.frame(pca_train$x[,1:338])
final_train['author']=author_train
# Form PCs similar to training dataset
loading_train <- pca_train$rotation[,1:338]

# multiply to get a test matrix with the principal component values
X_test_pc <- scale(X_test) %*% loading_train
final_test = data.frame(pca_test[,1:338])
final_test['author']=author_test

```
  

-------------------------------------------------------------------------------------

## LDA
  
  
-------------------------------------------------------------------------------------


```{r, warning=FALSE}
library(MASS)
lda_model <- lda(as.factor(author)~., data=final_train)

lda_pred <- predict(lda_model, newdata = final_test)

#Calculate the accuracy:

answer_lda <- as.data.frame(cbind(lda_pred, final_test$author))
answer_lda$correct <- ifelse(lda_pred$class==final_test$author, 1, 0)
sum(answer_lda$correct )*100/nrow(answer_lda)

```

-------------------------------------------------------------------------------------

## Naive Bayes model 

-------------------------------------------------------------------------------------
  

```{r echo = T, warning=FALSE}

naive_model =naiveBayes(as.factor(author) ~., data=final_train)
naive_pred = predict(naive_model,final_test)
# Calculating the accuracy
answer_naive <- as.data.frame(cbind(naive_pred, final_test$author))
answer_naive$correct <- ifelse(naive_pred==final_test$author, 1, 0)
sum(answer_naive$correct)*100/nrow(answer_naive)

#46%
```

-------------------------------------------------------------------------------------

## Random Forest 
  
-------------------------------------------------------------------------------------

```{r echo = T,warning=FALSE, message=FALSE}
library('randomForest')
rf_model = randomForest(as.factor(author) ~ ., data=final_train, ntree=1000, importance=TRUE)
rf_pred_test = predict(rf_model,final_test, type='response')
# Calculating the accuracy
answer <- as.data.frame(cbind(rf_pred_test, final_test$author))
answer$correct <- ifelse(rf_pred_test==final_test$author, 1, 0)
sum(answer$correct )*100/nrow(answer)
```
  

-------------------------------------------------------------------------------------
  
## KNN
  
-------------------------------------------------------------------------------------

```{R message=FALSE, warning=FALSE}
library(kknn)
accuracies = list()
for (i in c(7,9,11,13,15,17))
{
  knn_model = kknn(as.factor(author) ~ ., final_train, final_test,
                    distance = 1,
                    k= i,
                    kernel = 'rectangular')
  
  accuracies <- c(accuracies,sum(knn_model$fitted.values == final_test$author)/nrow(final_test))
}
plot(c(7,9,11,13,15,17), accuracies, main = "KNN accuracy vs K", xlab = "K-Values", ylab = "Accuracy Score", lty = 1)
```

-------------------------------------------------------------------------------------

Considering 10 nearest neighbours 

-------------------------------------------------------------------------------------

```{r,echo = FALSE}
# Build knn model with 10 nearest neightbours 

knn_model_vf = kknn(as.factor(author) ~ ., final_train, final_test,
                    distance = 1,
                    k= 10,
                    kernel = 'rectangular')

# calculate the accuracy 
pred_knn <- as.data.frame(cbind(knn_model$fitted.values, final_test$author))
pred_knn$correct <- ifelse(knn_model$fitted.values== final_test$author, 1, 0)

knn_acc= sum(pred_knn$correct)*100/nrow(pred_knn)
knn_acc

```


-------------------------------------------------------------------------------------

### Findings:

-------------------------------------------------------------------------------------

- LDA gave us the best accuracy (58.92%) among all the models we tried out (Naive Bayes, Random forest and KNN)
  
  
## Methodology:
<OL>
<LI>We removed terms which don't appear in 95% of the documents
<LI>We used Principle component analysis (PCA) to reduce the dimensions as including all the dimensions becomes computationally heavy However, accuracy is compromised due to reducing the number of dimensions 
</OL>

-------------------------------------------------------------------------------------

## Random Forest 
  
-------------------------------------------------------------------------------------

```{r echo = T,warning=FALSE, message=FALSE}
library('randomForest')
rf_model = randomForest(as.factor(author) ~ ., data=final_train, ntree=1000, importance=TRUE)
rf_pred_test = predict(rf_model,final_test, type='response')
# Calculating the accuracy
answer <- as.data.frame(cbind(rf_pred_test, final_test$author))
answer$correct <- ifelse(rf_pred_test==final_test$author, 1, 0)
sum(answer$correct )*100/nrow(answer)
```
  

-------------------------------------------------------------------------------------
  
## KNN
  
-------------------------------------------------------------------------------------

```{R message=FALSE, warning=FALSE}
library(kknn)
accuracies = list()
for (i in c(7,9,11,13,15,17))
{
  knn_model = kknn(as.factor(author) ~ ., final_train, final_test,
                    distance = 1,
                    k= i,
                    kernel = 'rectangular')
  
  accuracies <- c(accuracies,sum(knn_model$fitted.values == final_test$author)/nrow(final_test))
}
plot(c(7,9,11,13,15,17), accuracies, main = "KNN accuracy vs K", xlab = "K-Values", ylab = "Accuracy Score", lty = 1)
```

-------------------------------------------------------------------------------------

Considering 10 nearest neighbours 

-------------------------------------------------------------------------------------

```{r,echo = FALSE}
# Build knn model with 10 nearest neightbours 

knn_model_vf = kknn(as.factor(author) ~ ., final_train, final_test,
                    distance = 1,
                    k= 10,
                    kernel = 'rectangular')

# calculate the accuracy 
pred_knn <- as.data.frame(cbind(knn_model$fitted.values, final_test$author))
pred_knn$correct <- ifelse(knn_model$fitted.values== final_test$author, 1, 0)

knn_acc= sum(pred_knn$correct)*100/nrow(pred_knn)
knn_acc

```


-------------------------------------------------------------------------------------

### Findings:

-------------------------------------------------------------------------------------

- LDA gave us the best accuracy (58.92%) among all the models we tried out (Naive Bayes, Random forest and KNN)
  
  
## Methodology:
<OL><LI>We removed terms which don't appear in 95% of the documents
<LI> We used Principle component analysis (PCA) to reduce the dimensions as including all the dimensions becomes computationally heavy However, accuracy is compromised due to reducing the number of dimensions 
</OL>
-------------------------------------------------------------------------------------

# Quesion 6 - Association Rules

-------------------------------------------------------------------------------------

```{R message=FALSE, warning=FALSE}
# Association rule mining
rm(list=ls())

library(tidyverse)
library(arules)  
library(arulesViz)

groceries = read.csv('C:/Users/ameya/Documents/UT Austin/Academic/Intro to Predictive Modelling/STA380-master/STA380-master/data/groceries.txt', header=F, stringsAsFactors = F)
```

-------------------------------------------------------------------------------------

## EDA

-------------------------------------------------------------------------------------

```{R message=FALSE, warning=FALSE}
str(groceries)
# number of baskets
dim(groceries)[1]

# distribution of items sold
product_list = c(as.list(groceries['V1']), as.list(groceries['V2']),as.list(groceries['V3']), as.list(groceries['V4']))

product_list = unlist(product_list)
freq_of_products = as.data.frame(table(product_list))[-1,]
# total number of products sold
sum(freq_of_products['Freq'])
# number of unique products sold
dim(freq_of_products)[1]

# most popular products
data_plot = freq_of_products %>% arrange(-Freq) %>% head(20)

ggplot(data=data_plot,aes(x=reorder(product_list,Freq), y=Freq)) + 
  geom_bar(stat='identity') + 
    labs(title="Most popular products", 
       y="Frequency",
       x = "Products") +
  coord_flip() 

# least popular products
data_plot = freq_of_products %>% arrange(Freq) %>% head(20)

ggplot(data=data_plot,aes(x=reorder(product_list,-Freq), y=Freq)) + 
  geom_bar(stat='identity') + 
    labs(title="Least popular products", 
       y="Frequency",
       x = "Products") +
  coord_flip() 

```

-------------------------------------------------------------------------------------

### Findings:
<OL><LI> Most frequently bought item is whole milk
<LI> Number of unique items bought is 169
<LI>Total number of products sold is 43367
</OL>
-------------------------------------------------------------------------------------

## Association rules

```{R message=FALSE, warning=FALSE}
grocery_baskets = read.transactions(file="C:/Users/ameya/Documents/UT Austin/Academic/Intro to Predictive Modelling/STA380-master/STA380-master/data/groceries.txt", rm.duplicates=F, format="basket",sep=",",cols=1,header=F)

grocery_trans = as(grocery_baskets, "transactions")
summary(grocery_trans)

# Whole milk is the most popular product as confirmed in our EDA
groceryrules = apriori(grocery_trans, 
	parameter=list(support=.05, confidence=.1, maxlen=1))
                         
# Look at the output rules
arules::inspect(groceryrules)

groceryrules = apriori(grocery_trans, 
	parameter=list(support=.001, confidence=.50, maxlen=4))
                         
# Look at the output rules
arules::inspect(groceryrules)

plot(head(groceryrules, n = 10, by ="lift"), method = "graph", 
     main = "Top 10 Association Rules")

```

-------------------------------------------------------------------------------------

### Findings:
<OL><LI>Whole milk is the most commonly bought item which matches our findings from the EDA
<LI>People frequently add other salty snacks when they buy soda and popcorn
<LI>Liquor, blush wine and bottled beer are sold together
<LI>Baking items - flour, baking powder and sugar are bought together
<LI>Dairy products - Hard cheese, butter, whipped/sour cream and yogurt are bought together
<LI>White bread, domestic eggs and processed cheese sell together etc.
</OL>
-------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------

### Question 2

-------------------------------------------------------------------------------------

-------------------------------------------------------------------------------------

### Import the data and load the libraries

-------------------------------------------------------------------------------------

```{r message=FALSE}
abia=read.csv('C:/Users/ameya/Documents/UT Austin/Academic/Intro to Predictive Modelling/STA380-master/STA380-master/data/ABIA.csv')
abia1=abia
library("dplyr")
library(ggplot2)


```

-------------------------------------------------------------------------------------

## EDAs

```{r message=FALSE}
# Unique Identifier of flights : UniqueCarrier ,FlightNum , TailNum 
# Get the unique values in each column:
#sapply(abia, function(x) length(unique(x)))

# Number of flights per unique carrier 
num_flights=data.frame(abia %>% group_by(UniqueCarrier) %>% 
                         summarize(count=n_distinct(FlightNum))%>%
                         arrange(desc(count)))

#Plot bar chart :

ggplot(data = num_flights) + 
  geom_bar(mapping = aes(x = reorder(UniqueCarrier, -count), y=count), stat='identity',col="steelblue")

```

-------------------------------------------------------------------------------------

### Findings:
<ol><li> 16 Unique carriers
  <li>'WN' carrier has the highest number of flights , followed by carrier 'AA'
</ol>
  
-------------------------------------------------------------------------------------
  
## Traffic Analysis for the Airport
  
-------------------------------------------------------------------------------------
  

```{r message=FALSE}

########## Traffic Analysis for the Airport ##############
#Create a flag with 'Incoming' or 'Outgoing' flag 

abia$flight_type=if_else(abia$Origin=='AUS','Outgoing','Incoming')

#Trend :
#Monthly Trend :

flight_freq = abia %>%
  group_by(Month,flight_type)  %>%
  summarize(flight_Frequency = n()) 

abia$Month=as.factor(abia$Month)
ggplot(flight_freq, aes(x=factor(Month), y=flight_Frequency)) + 
  geom_bar(stat='identity')+ facet_wrap(~ flight_type, nrow = 2)


```

-------------------------------------------------------------------------------------

### Findings:
  
<ol><li> Summer is observed to be the most busy time for the airlines 
<li> Low traffic is observed during the winter
</ol>

-------------------------------------------------------------------------------------
  
## Weekly trend :
  
-------------------------------------------------------------------------------------

```{r message=FALSE}


abia$DayOfWeek=as.factor(abia$DayOfWeek)

week_freq = abia %>%
  group_by(DayOfWeek,flight_type)  %>%
  summarize(flight_Frequency = n()) 

ggplot(week_freq, aes(factor(DayOfWeek),flight_Frequency , fill = flight_type)) + 
  geom_bar(stat="identity", position = "dodge") + 
  scale_fill_brewer(palette = "Set1")

```

-------------------------------------------------------------------------------------

### Finding:
  
  - Less traffic is observed on Fridays (6th days of the week)

-------------------------------------------------------------------------------------

## Arrival and Departure Time Analysis

-------------------------------------------------------------------------------------

```{r message=FALSE}

abia$CRSDepTime <- sprintf("%04d", abia$CRSDepTime)
abia$CRSDepTime =format(strptime(abia$CRSDepTime , format="%H%M"), format = "%H:%M")
abia$CRSDepHour =format(strptime(abia$CRSDepTime , format="%H:%M"), format = "%H")

abia$CRSArrTime <- sprintf("%04d", abia$CRSArrTime)
abia$CRSArrTime =format(strptime(abia$CRSArrTime , format="%H%M"), format = "%H:%M")
abia$CRSArrHour =format(strptime(abia$CRSArrTime , format="%H:%M"), format = "%H")

# Hourly incoming and outgoing flight analysis

arr_freq = abia %>%
  group_by(CRSArrHour,flight_type)  %>%
  summarize(Arr_Frequency = n()) 

dep_freq = abia %>%
  group_by(CRSDepHour,flight_type)  %>%
  summarize(Dep_Frequency = n()) 

ggplot(data=arr_freq, aes(x=CRSArrHour, y=Arr_Frequency, group=flight_type)) +
  geom_line(aes(color=flight_type))+
  geom_point()

```

-------------------------------------------------------------------------------------

### Finding:
  
  - 8-9 AM (CRS) is observed to be peak arrival time, followed by evening 6PM, 4PM and 8PM 

-------------------------------------------------------------------------------------
  
## Peak Arrival Time

-------------------------------------------------------------------------------------

```{r message=FALSE}


ggplot(data=dep_freq, aes(x=CRSDepHour, y=Dep_Frequency, group=flight_type)) +
  geom_line(aes(color=flight_type))+
  geom_point()


```

-------------------------------------------------------------------------------------

### Finding:
  
  - 6-8 AM (CRS) is observed to be peak departure time for the flights

-------------------------------------------------------------------------------------

```{r message=FALSE}
#Box Plot :
ggplot(data=abia) + 
  geom_boxplot(mapping=aes(y=AirTime))

```

-------------------------------------------------------------------------------------

### Finding:
  
  - On an average flights have 100 mins of travel time from 1 destination to another

-------------------------------------------------------------------------------------
  
## Origin - Dest analysis 

-------------------------------------------------------------------------------------
  
```{r message=FALSE}


org_dest_flights=abia%>%
  group_by(Origin,Dest,flight_type)  %>%
  summarize(daily_flights = ceiling(n()/360) )

ggplot(org_dest_flights, aes(x=daily_flights)) +
  geom_histogram(binwidth=1, colour="blue", fill="white") 
```

-------------------------------------------------------------------------------------

```{r message=FALSE}
#Top incoming flights

inc_flights=org_dest_flights[which(org_dest_flights$flight_type=='Incoming'),]

top_inc=data.frame(head(arrange(inc_flights,desc(daily_flights)), n = 10))

ggplot(top_inc, aes(x=reorder(Origin,daily_flights),y=daily_flights)) +
  geom_bar(stat='identity') +
  coord_flip()

```

-------------------------------------------------------------------------------------

### Finding:
  
  - DFW, DAL and IAH are the high frequency incoming flights 

-------------------------------------------------------------------------------------

```{r message=FALSE}
#Top Outgoing Flights

out_flights=org_dest_flights[which(org_dest_flights$flight_type=='Outgoing'),]

top_out=data.frame(head(arrange(out_flights,desc(daily_flights)), n = 10))

ggplot(top_out, aes(x=reorder(Dest,daily_flights),y=daily_flights)) +
  geom_bar(stat='identity') +
  coord_flip()

```

-------------------------------------------------------------------------------------

### Finding:
  
  - DFW, DAL and IAH are the high frequency outgoing flights 
  

-------------------------------------------------------------------------------------

------------------------------------------------------------------------------------------- 

# Question 3

------------------------------------------------------------------------------------------- 
------------------------------------------------------------------------------------------- 

### Importing packages 

-------------------------------------------------------------------------------------------  


```{r message=FALSE}
library(mosaic)
library(quantmod)
library(foreach)
#### With more stocks
```

-----------------------------------------------------------------------------  
  
We have to create 3 portfolios. We are particularly interested in creating the following portfolios:
<OL>
<LI>Emerging Markets ETFs
<LI>Technology ETFs
<LI>Diversified ETFs
</OL>  
  
-----------------------------------------------------------------------------   

----------------------------------------------------------------------------- 
  
## Emerging Markets ETFs

-----------------------------------------------------------------------------   


```{R message=FALSE}
#1. A portfolio of Emerging Markets ETFs
#VWO : Vanguard FTSE Emerging Markets ETF
#IEMG : iShares Core MSCI Emerging Markets ETF
#EEM : iShares MSCI Emerging Markets ETF
#SCHE : Schwab Emerging Markets Equity ETF

mystocks = c("VWO", "IEMG", "EEM", "SCHE")
myprices = getSymbols(mystocks, from = "2014-01-01")



# A chunk of code for adjusting all stocks
# creates a new object adding 'a' to the end
# For example, WMT becomes WMTa, etc
for(ticker in mystocks) {
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text=expr))
}


# Combine all the returns in a matrix
all_returns = cbind(ClCl(VWOa),
								ClCl(IEMGa),
								ClCl(EEMa),
								ClCl(SCHEa))

all_returns = as.matrix(na.omit(all_returns))

summary(all_returns)
```
--------------------------------------------------------------------------------------

Out of all the ETFs, VWO seems like a bit more volatile than the rest (because of the spread) so let's assign it a slighly lower weight of 0.10 and assigning equal weights to the rest.

--------------------------------------------------------------------------------------

```{r message=False}
# Now simulate many different possible scenarios  
initial_wealth = 100000
weights = c(0.10, 0.30, 0.30, 0.30)
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
	total_wealth = initial_wealth
	holdings = weights * total_wealth
	n_days = 20
	wealthtracker = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(all_returns, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
		holdings = weights * total_wealth
	}
	wealthtracker
}

head(sim1)
hist(sim1[,n_days], 25)
# Profit/loss
mean(sim1[,n_days])
hist(sim1[,n_days]- initial_wealth, breaks=30,xlab = "Returns",main="Distribution of returns")

```

--------------------------------------------------------------------------------------
   
The tail risk at 5% level :
  
--------------------------------------------------------------------------------------
```{R message=FALSE}
quantile(sim1[,n_days]- initial_wealth, 0.05)
```

```{R message=FALSE}
#2. A portfolio of TECHNOLOGY ETFs

#XLK : Technology Select Sector SPDR Fund
#VGT : Vanguard Information Technology ETF
#IXN: iShares Global Tech ETF
#TDIV : First Trust NASDAQ Technology Dividend Index Fund

mystocks2 = c("XLK", "VGT", "IXN", "TDIV")
myprices2 = getSymbols(mystocks2, from = "2014-01-01")


for(ticker in mystocks2) {
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text=expr))
}


# Combine all the returns in a matrix
all_returns2 = cbind(	
                ClCl(XLKa),
								ClCl(VGTa),
								ClCl(IXNa),
								ClCl(TDIVa)
								)

all_returns2 = as.matrix(na.omit(all_returns2))

# Compute the returns from the closing prices
summary(all_returns2)
```

--------------------------------------------------------------------------------------
  
Again we use the spread of the data to assess the assignment of weights and we notice that XLK and VGT give higher returns at some point. Let's assign slighly higher weights to both these ETFs and create an aggressive portfolio and see how it works!
  
--------------------------------------------------------------------------------------

```{R message=FALSE}
# Now simulate many different possible scenarios  
initial_wealth = 100000
weights = c(0.30, 0.30, 0.20, 0.20) 
sim1 = foreach(i=1:1000, .combine='rbind') %do% {
	total_wealth = initial_wealth
	holdings = weights * total_wealth
	n_days = 20
	wealthtracker = rep(0, n_days)
	for(today in 1:n_days) 
	  {
		return.today = resample(all_returns, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
		holdings = weights * total_wealth
	  }
	wealthtracker
}

head(sim1)
hist(sim1[,n_days], 25)
# Profit/loss
mean(sim1[,n_days])
hist(sim1[,n_days]- initial_wealth, breaks=30,xlab = "Returns",main="Distribution of returns")
```

--------------------------------------------------------------------------------------

The tail risk at 5% level :

--------------------------------------------------------------------------------------
```{R message=FALSE}
quantile(sim1[,n_days]- initial_wealth, 0.05)
```
-----------------------------------------------------------------------------  

## 3. We will now look at diversified portfolios and assign equal weights to each of them to create a balanced portfolio

-----------------------------------------------------------------------------  

```{R message=FALSE}
#3. A portfolio of diversified ETFs

#AOR : iShares Core Growth Allocation ETF
#YYY : YieldShares High Income ETF
#MDIV : 	First Trust Multi-Asset Diversified Income Index Fund
#GAL : 		SPDR SSgA Global Allocation ETF

mystocks3 = c("AOR", "YYY", "GAL", "MDIV")
myprices3 = getSymbols(mystocks3, from = "2014-01-01")



# A chunk of code for adjusting all stocks
# creates a new object adding 'a' to the end
# For example, WMT becomes WMTa, etc
for(ticker in mystocks3) 
{
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text=expr))
}



# Combine all the returns in a matrix
all_returns3 = cbind(
								ClCl(AORa),
								ClCl(YYYa),
								ClCl(GALa),
								ClCl(MDIVa))


all_returns3 = as.matrix(na.omit(all_returns3))

# Compute the returns from the closing prices
summary(all_returns3)

```


```{R message=FALSE}
# Now simulate many different possible scenarios  
initial_wealth = 100000
weights = c(0.25, 0.25, 0.25, 0.25)
sim3 = foreach(i=1:5000, .combine='rbind') %do% {
	total_wealth = initial_wealth
	holdings = weights * total_wealth
	n_days = 20
	wealthtracker = rep(0, n_days)
	for(today in 1:n_days) {
		return.today = resample(all_returns, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
		holdings = weights * total_wealth
	}
	wealthtracker
}


hist(sim3[,n_days], 25)
# Profit/loss
mean(sim3[,n_days])
hist(sim3[,n_days]- initial_wealth, breaks=30,xlab = "Returns",main="Distribution of returns")
```

--------------------------------------------------------------------------------------
  
The tail risk at 5% level :

--------------------------------------------------------------------------------------
  
```{R message=FALSE}
quantile(sim3[,n_days]- initial_wealth, 0.05)
```


--------------------------------------------------------------------------------------
  
### Summary:
  
<OL><LI>We created three ETF portfolios
    <OL><LI> Emerging markets (stable because we assign a lower weight to the most volatile ETF)
    <LI>Technology sector ETFs (aggressive because we select the two most volatile high yielding ETFs and assign them higher weights than the rest)
    <LI>Diversified portfolio of ETFs (balanced because of equal weights)
    </OL>
    <LI> We then use bootrap resampling on each of these portfolios to assess the holdings and returns by redistributing the holdings in a fair way split at the end of each day. 
    <LI> We plot histograms for returns and find out the lower tail VaR for each portfolio at the 5% level
    </OL>
